ctransformers:
  model_path:
    small: "./models/mistral-7b-instruct-v0.1.Q3_K_M.gguf"
    large:  "./models/mistral-7b-instruct-v0.1.Q5_K_M.gguf"

  model_type: "mistral"
  model_config: 
    'max_new_tokens': 256
    'temperature' : 0.2
    'context_length': 4096
    'gpu_layers' : 0 # 32 to put all mistral layers on GPU, might differ for other models
    'threads' : -1

chat_config:
  chat_memory_length: 2
  number_of_retrieved_documents: 3

pdf_text_splitter:
  chunk_size: 1024 # number of characters 1024 roughly equels 256 tokens
  overlap: 50
  separators: ["\n", "\n\n"]

llava_model:
  llava_model_path: "models/llava/llava-v1.5-7b-Q4_K.gguf"
  clip_model_path: "models/llava/llava-v1.5-7b-mmproj-f16.gguf"

whisper_model: "openai/whisper-medium"

#embeddings_path: "BAAI/bge-large-v1.5"
#embeddings_path: "sentence-transformers/all-mpnet-base-v2"
#embeddings_path : "google-bert/bert-base-multilingual-cased"
embeddings_path : "FacebookAI/xlm-roberta-large"
chromadb:
  chromadb_path: "chroma_db"
  collection_name: "pdfs"

chat_sessions_database_path: "./chat_sessions"